现在，我们已经准备好描述我们CNN的总体结构。如图二所示，该网络包括了八个带权层；前五层是卷积层，剩下三层是全连接层。最后一个全连接层的输出被送到一个1000维的softmax层，softmax会产生一个覆盖1000类标签的分布。我们的网络使得多分类的Logistic回归目标最大化，这相当于最大化了预测分布下训练样本中正确标签的对数概率平均值。\\

第2,4,5卷积层的核只与位于同一GPU上的前一层的核映射相连接（看图2）。第3卷积层的核与第2层的所有核映射相连。全连接层的神经元与前一层的所有神经元相连。第1,2卷积层之后是响应归一化层。3.4节中描述的重叠max pooling层，跟在响应归一化层以及第五个卷积层之后。ReLU非线性激活函数应用于每个卷积层及全连接层的输出。\\

第1卷积层使用96个大小为$11\times11\times3$，步长为4个像素（这是同一核映射中邻近神经元的感受野中心之间的距离）的核，来对大小为$224\times224\times3$的输入图像进行滤波。第2层卷积层使用第1层卷积层的输出（响应归一化和池化过的）作为输入，并使用了256个大小为$5\times5\times48$的核进行滤波。第3,4,5层卷积层互相连接，中间没有接入池化层或者归一化层。第3层有384个大小为$3\times3\times256$核与第2层卷积层的输出（响应归一化和池化过的）相连。第4层卷积层有384个大小为$3\times3\times192$的核，第5层有256个大小为$3\times3\times192$的核。全连接层每个都有4096个神经元。\\