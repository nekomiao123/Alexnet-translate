我们使用随机梯度下降来训练我们的模型，样本的batch size是128，momentum是0.9，权重衰减为0.0005。我们发现这里少量的权重衰减对于模型的学习是很重要的。换句话说，权重衰减在这里不仅仅是一个正则化项：它减少了模型的训练误差。对于权重w的更新规则为\\
$$
v_{i+1} := 0.9 \cdot v_{i} - 0.0005\cdot \epsilon \cdot w_{i}-\epsilon \cdot \left \langle \frac{\partial L}{\partial w}\mid _{w_{i}} \right \rangle _{D_{i}}
$$

$$
w_{i+1}:=w_{i}+v_{i+1}
$$


我们使用均值为0，标准差为0.01的高斯分布对每一层的权重进行初始化。我们用常数1初始化了第二、四、五卷积层和全连接层的神经元的偏置项。这个初始化通过为ReLU提供正输入加速了学习的早期阶段。我们用常数0初始化了其余层神经元的偏置项。\\

我们对所有层都使用了相同的学习率，这是我们在训练过程中手动调整的。我们遵循的一些启发是：当验证误差率在当前学习率下不再提高是，就将学习率除以10。学习率初始化为0.01，在训练停止之前降低了三次。我们训练该网络时大致将这120万张图像的训练集循环了90次，在两个NVIDIA GTX 580 3GB GPU上花了五到六天。\\

