将许多不同模型的预测结合起来是降低测试误差[1, 3]的一个非常成功的方法，但这种方法对于本来就需要花费几天去训练的大型神经网络来说，代价太过高昂。然而，有一个非常有效的模型结合方法，它只花费两倍于单模型的训练成本。这种最新引入的技术，叫做“dropout”[10]，它以0.5的概率将每个隐层神经元的输出设置为0。这些“失活的”神经元既不在参与前向传播，也不再参与反向传播。所以每次输入时，神经网络会采用一个不同的架构，但是这所有的架构共享权重。这种技术降低了神经元之间复杂的互适应关系，因为一个神经元不能再依赖特定的神经元而存在了。因此，神经元被迫学习更具鲁棒性的特征，这些特征在结合其他神经元的一些不同随机子集时有用。在测试时，我们将所有的神经元的输出都乘以0.5，对指数级的许多失活网络的预测分布进行几何平均，这是一种合理的近似方法。\\

我们在图二的前两个全连接层中使用了dropout。如果没有dropout，我们的网络将会严重的过拟合。Dropout使得收敛所需要的迭代次数翻了一倍。