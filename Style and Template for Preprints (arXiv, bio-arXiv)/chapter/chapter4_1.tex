减少图像数据过拟合最简单最常用的方法，是使用标签-保留转换，人为地扩大数据集（例如，[25,4,5]）。我们使用了两种独特的数据增强方式，这两种方式都可以从原始图像通过非常少的计算量产生变换的图像，因此变换图像不需要存储在硬盘上。在我们的实现中，变换图像通过CPU的Python代码生成，而此时GPU正在训练前一批图像。所以这种放大数据集的方式是很高效很节省计算资源的。\\

第一种数据增强的方式包括图像变换和水平翻转。我们通过从$256\times256$的图像上随机提取$224\times224$的图像块（及其水平镜像）的方法来实现，并在这些提取的图像$\footnote{这就是在图2中输入为224×224×3的原因.}$上对我们的神经网络进行了训练。这通过一个2048因子增大了我们的训练集【？ This increases the size of our training set by a factor of 2048】，尽管最终的训练样本是高度相关的。如果没有这个方案，我们的网络会出现严重的过拟合，这会迫使我们使用更小的网络。在测试时，网络会抽取五个$224\times224$的图像块（四个角上的图像块和中心的图像块）以及他们的水平翻转（总共是个图像块）进行预测，然后将softmax层对这十个图像块做出的预测取平均。\\

第二种放大数据集的方法是改变训练图像的RGB通道的强度。具体地，我们在整个ImageNet训练集上对RGB像素进行了PCA（主成分分析）。对于每张训练图像，我们通过均值为0，方差为0.1的高斯分布产生一个随机值a，然后通过向图像中加入更大比例的相应的本征值的a倍，把其主成分翻倍。因此，对于每个RGB像素$I_{xy}=\begin{bmatrix}
I_{xy}^{R} & I_{xy}^{G} & I_{xy}^{B} 
\end{bmatrix}^{T}$，我们加入的值如下：\\
$$
\begin{bmatrix}
P_{1} & P_{2} & P_{3}
\end{bmatrix}
\begin{bmatrix}
\alpha _{1}\lambda _{1} & \alpha_{2}\lambda _{2} & \alpha _{3}\lambda _{3}
\end{bmatrix}^{T}
$$

其中，$P_{i}$和$\lambda _{i}$分别是RGB像素值$3\times3$协方差矩阵的第$i$个特征向量和特征值，$\alpha ^{i}$是前面提到的随机变量。对于某个训练图像的所有像素，每个$\alpha ^{i}$只获取一次，直到图像进行下一次训练时才重新获取。这个方案近似抓住了自然图像的一个重要特性，即光照的颜色和强度发生变化时，目标是不变的。这一方法把top-1错误降低了1\%。
