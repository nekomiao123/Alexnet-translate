ReLU具有让人满意的特性，它们无需对输入数据进行归一化（归一化的作用是来防止它们饱和）。如果至少一些训练样本对ReLU产生了正输入，那么学习将发生在那个神经元上。然而，我们仍然发现接下来的局部响应归一化有助于泛化。用$a_{x,y}^{i}$来表示在点$(x,y)$处通过先应用核$i$计算，然后再应用ReLU非线性计算出来的神经元激活度，响应归一化活性$b_{x,y}^{i}$由下面的式子给出
$$
b_{x,y}^{i}=a_{x,y}^{i}/\left ( k+\alpha \sum _{j=max(0,i-n/2)}^{min(N-1,i+n/2)}(a_{x,y}^{j})^{2} \right )^{\beta }
$$
求和运算在n个“毗邻的”核映射的同一位置上执行，N是本层的卷积核数目。核映射的顺序是任意的，这在训练开始前就确定了。响应归一化的顺序实现的是一种侧抑制形式，其灵感来源于真实神经元中的情况，这使得在使用不同核计算神经元输出的过程中创造了对大激活度的竞争。常量$k$,$n$,$\alpha$,$\beta$都是超参数，它们的值由验证集确定;我们设定$k=2$,$n=5$,$\alpha=10^{-4}$,$\beta=0.75$。我们在特定的层使用的ReLU非线性激活函数之后应用了这种归一化（请看3.5小节）。\\

该方案与Jarrett等人的局部对比度归一化方案具有一些相似之处[11]，但是我们更正确的名称应该是“亮度归一化”，因为我们不减去平均活跃度。响应归一化将我们的top-1与top-5误差率分别减少了1.4\%与1.2\%。我们也验证了该方案在CIFAR-10数据集上的有效性：四层CNN不带归一化时的测试误差率是13\%，带归一化时是11\%$\footnote{由于篇幅有限，我们无法在这里十分详细的描述神经网络，更详细的参见代码以及代码当中的超参数，在这个网站你可以找到 http://code.google.com/p/cuda-convnet/.}$。