对神经元输出$f$的标准建模方法是将输入$x$函数变换为$f(x) = tanh(x)$或$f(x) = (1+e^{-x})^{-1}$。就梯度下降法的训练时间而言，这些饱和非线性函数比非饱和非线性函数如$f(x)=max(0,x)$要慢得多。根据Nair和Hinton的说法[20]，我们将这种非线性单元称为修正非线性单元（Rectified Linear Units (ReLUs)）。用ReLU作为激活函数的卷积神经网络比起来同等规模使用tanh作为激活函数的卷积神经网络训练速度快了好几倍。这个结果可以从图一中看出来，该图展示了对于一个特定的四层CNN，在CIFAR-10数据集上达到25\%的训练误差所需的迭代次数。这张图说明，如果我们采用传统的饱和神经元，我们不可能为这项工作训练如此庞大的神经网络。\\

我们并不是第一个考虑在CNN中替换掉传统神经元模型的人。例如，Jarrett等人[11] 声称，用非线性函数$f(x)=|tanh(x)|$在Caltech-101数据集上做对比度归一化（Contrast Normalization，CN）和局部平均值池化表现得很好。然而，在这个数据集中，主要担心的还是过拟合，所以他们观察到的效果与我们在使用ReLU时观察到的训练集的加速能力不一样。加快训练速度对于在大型数据集上训练大型模型的表现有重大的影响。