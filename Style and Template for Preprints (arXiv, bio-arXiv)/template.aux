\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}介绍}{1}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}数据集}{2}{section.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}模型体系结构}{2}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}ReLU非线性函数}{2}{subsection.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces 使用ReLU的四层卷积神经网络（实线）在CIFAR-10数据集上达到25\%的训练误差比使用tanh神经元的等价网络（虚线）快六倍。为了使训练尽可能快，每个网络的学习率是独立选取的。没有采用任何类型的正则化。这里演示的效果因网络结构的不同而不同，但带ReLU的网络学习始终比带饱和神经元的同等网络快好几倍。}}{3}{figure.1}}
\newlabel{fig:fig1}{{1}{3}{使用ReLU的四层卷积神经网络（实线）在CIFAR-10数据集上达到25\%的训练误差比使用tanh神经元的等价网络（虚线）快六倍。为了使训练尽可能快，每个网络的学习率是独立选取的。没有采用任何类型的正则化。这里演示的效果因网络结构的不同而不同，但带ReLU的网络学习始终比带饱和神经元的同等网络快好几倍。}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}多GPU并行训练}{3}{subsection.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}局部响应归一化}{3}{subsection.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}重叠池化}{4}{subsection.3.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}总体结构}{4}{subsection.3.5}}
\@writefile{toc}{\contentsline {section}{\numberline {4}减少过拟合}{4}{section.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces CNN体系结构示意图，明确显示了两个GPU之间的职责划分。一个GPU运行图中顶部的层次部分，而另一个GPU运行图中底部的层次部分。GPU之间仅在某些层互相通信。该网络的输入是150,528维的，且该网络剩下各层的神经元数分别为253,440–186,624–64,896–64,896–43,264–4096–4096–1000。}}{5}{figure.2}}
\newlabel{fig:fig2}{{2}{5}{CNN体系结构示意图，明确显示了两个GPU之间的职责划分。一个GPU运行图中顶部的层次部分，而另一个GPU运行图中底部的层次部分。GPU之间仅在某些层互相通信。该网络的输入是150,528维的，且该网络剩下各层的神经元数分别为253,440–186,624–64,896–64,896–43,264–4096–4096–1000。}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}数据增强}{5}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}随机失活}{6}{subsection.4.2}}
\@writefile{toc}{\contentsline {section}{\numberline {5}学习细节}{6}{section.5}}
\@writefile{toc}{\contentsline {section}{\numberline {6}结果}{6}{section.6}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces ILSVRC-2010测试集上的结果比较。斜体字是他人取得的最好结果。}}{7}{table.1}}
\newlabel{tab:table}{{1}{7}{ILSVRC-2010测试集上的结果比较。斜体字是他人取得的最好结果。}{table.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces ILSVRC-2010测试集上的结果比较。斜体字是他人取得的最好结果。}}{7}{table.2}}
\newlabel{tab:table}{{2}{7}{ILSVRC-2010测试集上的结果比较。斜体字是他人取得的最好结果。}{table.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces 第一卷积层在224×224×3的输入图像上学习到的大小为11×11×3的96个卷积核。上面的48个核是在GPU 1上学习到的而下面的48个卷积核是在GPU 2上学习到的。更多细节请看6.1小节。}}{7}{figure.3}}
\newlabel{fig:fig3}{{3}{7}{第一卷积层在224×224×3的输入图像上学习到的大小为11×11×3的96个卷积核。上面的48个核是在GPU 1上学习到的而下面的48个卷积核是在GPU 2上学习到的。更多细节请看6.1小节。}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}定性评价}{8}{subsection.6.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces (左）8张ILSVRC-2010测试图像和我们的模型认为最可能的5个标签。每张图像的下面是它的正确标签，正确标签的概率用红条表示（如果正确标签在前五当中）。（右）第一列是5张ILSVRC-2010测试图像。剩下的列展示了6张训练图像，这些图像的最后的隐藏层的特征向量与测试图像的特征向量有最小的欧氏距离。}}{8}{figure.4}}
\newlabel{fig:fig4}{{4}{8}{(左）8张ILSVRC-2010测试图像和我们的模型认为最可能的5个标签。每张图像的下面是它的正确标签，正确标签的概率用红条表示（如果正确标签在前五当中）。（右）第一列是5张ILSVRC-2010测试图像。剩下的列展示了6张训练图像，这些图像的最后的隐藏层的特征向量与测试图像的特征向量有最小的欧氏距离。}{figure.4}{}}
\bibstyle{unsrt}
\bibcite{1}{1}
\bibcite{2}{2}
\bibcite{3}{3}
\bibcite{4}{4}
\bibcite{5}{5}
\bibcite{6}{6}
\bibcite{7}{7}
\bibcite{8}{8}
\bibcite{9}{9}
\bibcite{10}{10}
\bibcite{11}{11}
\bibcite{12}{12}
\bibcite{13}{13}
\bibcite{14}{14}
\bibcite{15}{15}
\bibcite{16}{16}
\bibcite{17}{17}
\bibcite{18}{18}
\@writefile{toc}{\contentsline {section}{\numberline {7}讨论}{9}{section.7}}
\bibcite{19}{19}
\bibcite{20}{20}
\bibcite{21}{21}
\bibcite{22}{22}
\bibcite{23}{23}
\bibcite{24}{24}
\bibcite{25}{25}
\bibcite{26}{26}
